{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca0477a6-cdff-4b62-8580-826911c4c29a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Medium scraper\n",
    "\n",
    "Script to scrape Medium articles based on tag and date range.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bbf103-ac12-417c-95e2-5ec8b53667ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28ca8e49-e699-4c41-9b0f-d91f583e4969",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import date,timedelta\n",
    "import time\n",
    "import csv\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97295201-a298-4f7a-be5c-f01ec4a4d006",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#PARAMETERS. Change as needed\n",
    "\n",
    "#Tags to scrape from\n",
    "TAGS = [\n",
    "        'data-science'\n",
    "]\n",
    "\n",
    "#Date range to scrape data\n",
    "DATE_FROM = date(2022,1,1) \n",
    "DATE_TO = date(2022,1,31)\n",
    "\n",
    "#time interval for scraping\n",
    "INTERVAL = 3\n",
    "\n",
    "# Maximum number of articles to scrape per day\n",
    "MAX_ARTICLES_PER_DAY = 30 \n",
    "MAX_RETRIES = 3\n",
    "\n",
    "#email address used to sign in to medium (to scrape member-only articles)\n",
    "MEDIUM_EMAIL = 'hiroakiroa@gmail.com'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75c7737-7aa3-4fc2-9e7d-6b3e67971c75",
   "metadata": {},
   "source": [
    "### Initialize Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36868ff0-b979-4bb5-97db-c06e7e12c3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're running this through colab, you can use the following script instead of the next one. \n",
    "\n",
    "# %%shell\n",
    "# sudo apt -y update\n",
    "# sudo apt install -y wget curl unzip\n",
    "# wget http://archive.ubuntu.com/ubuntu/pool/main/libu/libu2f-host/libu2f-udev_1.1.4-1_all.deb\n",
    "# dpkg -i libu2f-udev_1.1.4-1_all.deb\n",
    "# wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
    "# dpkg -i google-chrome-stable_current_amd64.deb\n",
    "\n",
    "# wget -N https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/118.0.5993.70/linux64/chromedriver-linux64.zip -P /tmp/\n",
    "# unzip -o /tmp/chromedriver-linux64.zip -d /tmp/\n",
    "# chmod +x /tmp/chromedriver-linux64/chromedriver\n",
    "# mv /tmp/chromedriver-linux64/chromedriver /usr/local/bin/chromedriver\n",
    "# pip install selenium chromedriver_autoinstaller\n",
    "\n",
    "# import sys\n",
    "# sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
    "\n",
    "# from selenium import webdriver\n",
    "# import chromedriver_autoinstaller\n",
    "\n",
    "# chrome_options = webdriver.ChromeOptions()\n",
    "# chrome_options.add_argument('--headless') # this is must\n",
    "# chrome_options.add_argument('--no-sandbox')\n",
    "# chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "# chromedriver_autoinstaller.install()\n",
    "\n",
    "# driver = webdriver.Chrome(options=chrome_options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "386492b5-bcaf-4c08-b666-7c4a6ea83c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're running on your PC, the following will open a new chrome instance/window unless you add --headless argument\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "#chrome_options.add_argument('--headless') #uncomment this if you don't want to see the chrome browser\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "driver = webdriver.Chrome(options=chrome_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b881e958-e0ed-4514-984f-26e7b065484e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Scrape article list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef8aee89-0e05-4214-8f1f-de31cf8cf36e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-01 2022-01-02 2022-01-03 2022-01-04 2022-01-05 2022-01-06 2022-01-07 2022-01-08 2022-01-09 2022-01-10 2022-01-11 2022-01-12 2022-01-13 2022-01-14 2022-01-15 2022-01-16 2022-01-17 2022-01-18 2022-01-19 2022-01-20 2022-01-21 2022-01-22 2022-01-23 2022-01-24 2022-01-25 2022-01-26 2022-01-27 2022-01-28 2022-01-29 2022-01-30 2022-01-31 "
     ]
    }
   ],
   "source": [
    "#Initialize\n",
    "curr_date = DATE_FROM\n",
    "delta = timedelta(days=1)\n",
    "df = pd.DataFrame(columns=[\"url\",\"tag\",\"date\"])\n",
    "\n",
    "\n",
    "with open(\"data/article_list.csv\", \"w\") as file:\n",
    "    file.write(\"url,tag,date\\n\")\n",
    "    for tag in TAGS:\n",
    "        while curr_date <= DATE_TO:\n",
    "            print(curr_date, end=\" \")\n",
    "            retry = 0\n",
    "            response = None\n",
    "            driver.get('https://medium.com/tag/' + tag + \"/archive/\" +curr_date.strftime(\"%Y/%m/%d\"))\n",
    "            time.sleep(INTERVAL)\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "            anchor_tags = soup.find_all('a')\n",
    "            for anchor in anchor_tags:\n",
    "                # As of Mar 2024 it seems the article titles (which are hyperlinks) are set as h3. Using this as the tag to find the article urls\n",
    "                if (anchor.find('h3') is not None):\n",
    "                    href = anchor.get('href')\n",
    "                    href = re.findall(\"[^\\?]+\", href)[0]\n",
    "                    file.write(f'\"{href}\",\"{tag}\",\"{curr_date.strftime(\"%Y-%m-%d\")}\"\\n')\n",
    "            curr_date += delta\n",
    "            \n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2316ae-815a-4883-9ef2-5da4b72fdef6",
   "metadata": {},
   "source": [
    "## Scrape articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e231b5-0812-4a5c-8a86-1233aaa59752",
   "metadata": {},
   "source": [
    "### Medium sign in\n",
    "This step is required to scrape member articles. Note that the script interacts with the browser window using  selinium, which makes it more vulnerable to Medium site changes. \n",
    "\n",
    "__Make sure you have updated the MEDIUM_EMAIL parameter__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9fb1a9f6-c2c0-46d1-bcc9-919086550db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://medium.com\")\n",
    "driver.implicitly_wait(3)\n",
    "element = driver.find_element(By.XPATH,\"//button[text()='Sign in']\")\n",
    "element.click()\n",
    "driver.implicitly_wait(0.5)\n",
    "sign_in_with_email = driver.find_element(By.XPATH,'//div[text()=\"Sign in with email\"]')\n",
    "\n",
    "sign_in_with_email.click()\n",
    "driver.implicitly_wait(0.5)\n",
    "email = driver.find_element(By.XPATH,\"//input[@aria-label='email']\")\n",
    "email.click()\n",
    "driver.implicitly_wait(0.5)\n",
    "\n",
    "email.send_keys(MEDIUM_EMAIL)\n",
    "driver.implicitly_wait(1)\n",
    "cont = driver.find_element(By.XPATH,'//button[text()=\"Continue\"]')\n",
    "cont.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11e2cfc-d0e8-400f-b6ef-a0326b198dc8",
   "metadata": {},
   "source": [
    "The script above should send a login verification email to your email address. Once you receive the email address, copy the URL and paste to the _signin_url_ field below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5535ccd8-d0cf-4e57-a376-c881c3f59f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "signin_url = 'https://medium.com/m/callback/email?token=b1209596ac2a&operation=login&state=medium&source=email-bcc399e9fcdc-1713391728621-auth.login------0-------------------3ef35ceb_9902_4689_bce6_da88396ccb03'\n",
    "driver.get(signin_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7b4fe6-6fec-4df3-9c95-fe3e86acc954",
   "metadata": {},
   "source": [
    "### Scrape articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cbd86861-ad5f-477a-8ca0-db5efcadcf51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-01 2022-01-02 2022-01-03 2022-01-04 2022-01-05 2022-01-06 2022-01-07 2022-01-08 2022-01-09 2022-01-10 2022-01-11 2022-01-12 2022-01-13 2022-01-14 2022-01-15 2022-01-16 2022-01-17 2022-01-18 2022-01-19 2022-01-20 2022-01-21 2022-01-22 2022-01-23 2022-01-24 2022-01-25 2022-01-26 2022-01-27 2022-01-28 2022-01-29 2022-01-30 2022-01-31 "
     ]
    }
   ],
   "source": [
    "# If you don't want to scrape all the articles in the article_list, filter the days field below. \n",
    "# Note that the date field is stored as a string in the format of  YYYY-mm-DD\n",
    "# article_list_df = article_list_df.loc[(article_list_df['date'] >= ('2023-09'))] \n",
    "\n",
    "\n",
    "article_list_df = pd.read_csv('data/article_list.csv')\n",
    "\n",
    "#Initialize\n",
    "date_tag_df = pd.concat([article_list_df['date'].str.slice(0, 7),article_list_df[['tag']]],axis=1).drop_duplicates().reset_index(drop=True)\n",
    "date_tag_df = date_tag_df.rename(columns={'date':'year-month'})\n",
    "\n",
    "#Creating separate file per year-month + tag pair\n",
    "for index,date_tag_row in date_tag_df.iterrows():\n",
    "    with open(f\"data/articles_{date_tag_row['year-month']}_{date_tag_row['tag']}.csv\", \"w\",encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file, dialect='excel')\n",
    "        writer.writerow(['url','tag','date','content'])\n",
    "\n",
    "        #Get list of scrapable articles filtered by year-month + tag\n",
    "        month_article_df = article_list_df.loc[(article_list_df['date'].str.contains(date_tag_row['year-month'])) & (article_list_df['tag'] == date_tag_row['tag']),:]\n",
    "        for day in month_article_df['date'].unique():\n",
    "            print(day, end=\" \")\n",
    "            # If number of articles is more than the MAX_ARTICLES_PER_DAY, sample\n",
    "            if month_article_df.loc[month_article_df['date']==day,:].shape[0] > MAX_ARTICLES_PER_DAY:\n",
    "              sampled_urls = month_article_df.loc[month_article_df['date']==day,:].sample(n=MAX_ARTICLES_PER_DAY,replace=False,random_state=0)\n",
    "            else:\n",
    "              sampled_urls = month_article_df.loc[article_list_df['date']==day,:]\n",
    "\n",
    "            \n",
    "            for index, row in sampled_urls.iterrows():\n",
    "                url= row['url']\n",
    "                tag= row['tag']\n",
    "                article_date = row['date']\n",
    "                content = \"\"\n",
    "                retry = 0\n",
    "                response = None\n",
    "                try:\n",
    "                    driver.get(url)\n",
    "                    time.sleep(INTERVAL)\n",
    "    \n",
    "                except Exception as e:\n",
    "                        print(\"An error occurred, moving to next article\", e)\n",
    "                        continue\n",
    "                    \n",
    "                html = driver.page_source\n",
    "                soup = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "                article = soup.find('article')\n",
    "                if article is not None:\n",
    "                    # The script assumes all the texts with <p> tag are the article's main text. This can definitely be improved. \n",
    "                    for para in article.find_all(\"p\"): \n",
    "                        content += para.get_text() + chr(10)\n",
    "                    writer.writerow([url,tag,article_date,content])\n",
    "    file.close()\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500175a3-ed9f-4b3b-ac9e-414cb70b0ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
